# üìö DocMind RAG ‚Äî Chat with Any PDF using AI

> **Upload any PDF and ask questions** ‚Äî get accurate, page-cited answers powered by Retrieval-Augmented Generation (RAG) with HuggingFace LLMs, FastEmbed, and in-memory Qdrant.

<!-- Add your demo GIF/screenshot here after deploying -->
<!-- ![Demo](docs/demo.gif) -->

**üîó [Live Demo](https://ragdeployedtest.streamlit.app/) ¬∑ [Get Free HuggingFace Token](https://huggingface.co/settings/tokens)**

---

## ‚ú® Key Features

| Feature | Details |
|---|---|
| **Upload Any PDF** | Users upload their own PDF ‚Äî no pre-indexed data needed |
| **BYOK (Bring Your Own Key)** | Each user provides their own free Gemini API key ‚Äî your API key stays safe |
| **In-Memory Processing** | PDF is chunked, embedded, and stored in-memory ‚Äî nothing is saved after the session |
| **Smart Chunking** | Recursive splitting (1 000 tokens, 400-token overlap) preserves context across pages |
| **Fast Embeddings** | `BAAI/bge-small-en-v1.5` (384-dim) via FastEmbed ‚Äî runs on CPU, zero API cost |
| **Page Citations** | Every answer references the exact page number for easy verification |
| **ChatGPT-style UI** | Clean, dark-themed chat interface built with Streamlit |
| **Privacy-first** | No data stored, no API keys saved ‚Äî everything dies when you close the tab |
| **CLI Tools** | Bonus: CLI scripts for local batch indexing and terminal-based chat |

---

## üèóÔ∏è Architecture

```
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚îÇ            Streamlit Web App                ‚îÇ
                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                            ‚îÇ
                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                     ‚ñº                      ‚ñº                      ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  User ‚îÄ‚îÄ‚ñ∂   ‚îÇ  Upload PDF ‚îÇ      ‚îÇ  User Query   ‚îÇ       ‚îÇ  Gemini Key  ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ                    ‚îÇ                       ‚îÇ
                    ‚ñº                    ‚ñº                       ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
           ‚îÇ  PyPDF Load   ‚îÇ    ‚îÇ     Embed Query   ‚îÇ             ‚îÇ
           ‚îÇ  + Chunking   ‚îÇ    ‚îÇ  (FastEmbed/CPU)  ‚îÇ             ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
                  ‚îÇ                     ‚îÇ                       ‚îÇ
                  ‚ñº                     ‚ñº                       ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
           ‚îÇ   FastEmbed   ‚îÇ    ‚îÇ  Cosine Search    ‚îÇ             ‚îÇ
           ‚îÇ  (bge-small)  ‚îÇ    ‚îÇ  Qdrant In-Memory ‚îÇ             ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
                  ‚îÇ                     ‚îÇ                       ‚îÇ
                  ‚ñº                     ‚ñº                       ‚ñº
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ Qdrant Store  ‚îÇ    ‚îÇ  System Prompt + Context + Query ‚îÇ
           ‚îÇ  (in-memory)  ‚îÇ    ‚îÇ         ‚Üí Google Gemini LLM      ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚îÇ
                                              ‚ñº
                                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                     ‚îÇ   Answer +    ‚îÇ
                                     ‚îÇ  Page Cited   ‚îÇ
                                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Tech Stack

| Layer | Technology |
|---|---|
| Language | Python 3.10+ |
| Framework | LangChain |
| Frontend | Streamlit (ChatGPT-style UI) |
| Vector DB | Qdrant (in-memory for web app / Docker for CLI) |
| Embeddings | FastEmbed ‚Äî `BAAI/bge-small-en-v1.5` (384-dim, CPU) |
| LLM | HuggingFace Inference API (Mistral 7B, Zephyr 7B, Phi-3, Qwen 2.5) |
| PDF Loader | PyPDF |

---

## üìÇ Project Structure

```
.
‚îú‚îÄ‚îÄ app.py                    # üåê Streamlit web UI (upload PDF + chat)
‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îú‚îÄ‚îÄ index.py              # CLI: Ingest PDF ‚Üí chunk ‚Üí embed ‚Üí Qdrant
‚îÇ   ‚îú‚îÄ‚îÄ chat.py               # CLI: Single-query RAG chat
‚îÇ   ‚îú‚îÄ‚îÄ chat_autorun.py       # CLI: Interactive multi-turn chat loop
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml    # One-command Qdrant setup (for CLI mode)
‚îú‚îÄ‚îÄ .streamlit/
‚îÇ   ‚îî‚îÄ‚îÄ config.toml           # Streamlit theme (dark mode)
‚îú‚îÄ‚îÄ .env.example              # Template for environment variables
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üöÄ Quick Start

### Option A: Web UI (Recommended)

```bash
git clone https://github.com/tejaswisinghparmar/RAG.git
cd RAG
python -m venv venv && source venv/bin/activate  # Windows: .\venv\Scripts\Activate.ps1
pip install -r requirements.txt
streamlit run app.py
```

Open **http://localhost:8501** ‚Üí paste your [free Gemini API key](https://aistudio.google.com/apikey) ‚Üí upload a PDF ‚Üí start chatting!

### Option B: CLI Mode (with Docker Qdrant)

```bash
# 1. Start Qdrant
cd rag && docker compose up -d && cd ..

# 2. Configure
cp .env.example rag/.env
# Edit rag/.env with your GOOGLE_API_KEY

# 3. Index a PDF
python rag/index.py

# 4. Chat
python rag/chat_autorun.py
```

---

## ‚òÅÔ∏è Free Deployment (Streamlit Community Cloud)

The web app needs **zero external services** ‚Äî no database, no paid APIs.

| What | Where | Cost |
|---|---|---|
| Web App | [Streamlit Community Cloud](https://streamlit.io/cloud) | Free (public repos) |
| LLM | Users bring their own [Gemini key](https://aistudio.google.com/apikey) | Free for users |
| Vector DB | In-memory (no setup needed) | Free |
| Embeddings | FastEmbed (runs on CPU) | Free |

### Deploy in 3 Steps

1. **Push to GitHub** ‚Äî Make sure your repo is public
2. **Go to [share.streamlit.io](https://share.streamlit.io/)** ‚Üí Connect your GitHub repo ‚Üí Set main file to `app.py`
3. **Done!** Share the URL on your resume. No secrets needed ‚Äî users bring their own key.

---

## üîí Security & Privacy

- **BYOK Model** ‚Äî Users enter their own Gemini API key. Your key is never exposed.
- **No Persistent Storage** ‚Äî PDFs are processed in-memory and discarded when the session ends.
- **API Keys Not Stored** ‚Äî Keys exist only in the browser session state.
- **`.gitignore` Protection** ‚Äî `.env` files and PDFs are excluded from version control.

---

## üí° How It Works

1. **Upload** ‚Äî User uploads a PDF via the Streamlit sidebar.
2. **Chunk** ‚Äî The PDF is split into overlapping chunks (~1 000 tokens each, 400-token overlap) to preserve context across page boundaries.
3. **Embed** ‚Äî Each chunk is embedded using `bge-small-en-v1.5` (384-dim vectors) running locally on CPU via FastEmbed.
4. **Store** ‚Äî Vectors are stored in an in-memory Qdrant instance (no external database).
5. **Retrieve** ‚Äî When the user asks a question, the query is embedded and the top-4 most similar chunks are retrieved via cosine similarity.
6. **Generate** ‚Äî Retrieved chunks + page numbers are injected into a system prompt, and Google Gemini generates a grounded, cited answer.

---

## üß† Challenges & Learnings

- **Chunking strategy matters** ‚Äî Recursive splitting with 400-token overlap significantly improved retrieval accuracy for questions spanning multiple pages.
- **FastEmbed vs cloud embeddings** ‚Äî Switched from cloud-based embedding APIs to FastEmbed for zero-cost, offline-capable, and faster indexing on CPU.
- **BYOK for free deployment** ‚Äî Instead of burning through a shared API quota, letting users bring their own key makes the app sustainably free.
- **Prompt engineering for citations** ‚Äî Explicitly instructing the LLM to cite page numbers reduced hallucinated answers and improved verifiability.
- **In-memory Qdrant** ‚Äî Using `:memory:` mode eliminates the need for an external database in deployment while keeping the same LangChain API.

---

## üîÆ Future Scope

- **Agentic RAG** ‚Äî Add tool-calling to let the LLM decide when to search, summarise, or ask for clarification
- **Hybrid Retrieval** ‚Äî Combine dense (vector) + sparse (BM25) search for better recall
- **Reranking** ‚Äî Cross-encoder reranker (e.g., `ms-marco-MiniLM`) for improved precision
- **Multi-document support** ‚Äî Upload multiple PDFs and filter by source at retrieval time
- **Streaming responses** ‚Äî Token-by-token output for a more responsive feel
- **Evaluation with RAGAS** ‚Äî Measure faithfulness, answer relevance, and context precision
- **Chat history export** ‚Äî Download conversation as PDF/Markdown

---

## üìú License

This project is open-source under the [MIT License](LICENSE).

---

> Built with ‚ù§Ô∏è using LangChain, Qdrant, FastEmbed, HuggingFace, and Streamlit.
