# ğŸ“š DocMind RAG â€” Chat with Any PDF using AI

> **Upload any PDF and ask questions** â€” get accurate, page-cited answers powered by Retrieval-Augmented Generation (RAG) with Google Gemini, FastEmbed, and in-memory Qdrant.

<!-- Add your demo GIF/screenshot here after deploying -->
<!-- ![Demo](docs/demo.gif) -->

**ğŸ”— [Live Demo](#) Â· [Get Free Gemini API Key](https://aistudio.google.com/apikey)**

---

## âœ¨ Key Features

| Feature | Details |
|---|---|
| **Upload Any PDF** | Users upload their own PDF â€” no pre-indexed data needed |
| **BYOK (Bring Your Own Key)** | Each user provides their own free Gemini API key â€” your API key stays safe |
| **In-Memory Processing** | PDF is chunked, embedded, and stored in-memory â€” nothing is saved after the session |
| **Smart Chunking** | Recursive splitting (1 000 tokens, 400-token overlap) preserves context across pages |
| **Fast Embeddings** | `BAAI/bge-small-en-v1.5` (384-dim) via FastEmbed â€” runs on CPU, zero API cost |
| **Page Citations** | Every answer references the exact page number for easy verification |
| **ChatGPT-style UI** | Clean, dark-themed chat interface built with Streamlit |
| **Privacy-first** | No data stored, no API keys saved â€” everything dies when you close the tab |
| **CLI Tools** | Bonus: CLI scripts for local batch indexing and terminal-based chat |

---

## ğŸ—ï¸ Architecture

```
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚            Streamlit Web App                â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â–¼                      â–¼                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  User â”€â”€â–¶   â”‚  Upload PDF â”‚      â”‚  User Query   â”‚       â”‚  Gemini Key  â”‚
              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                    â”‚                       â”‚
                    â–¼                    â–¼                       â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
           â”‚  PyPDF Load   â”‚    â”‚     Embed Query   â”‚             â”‚
           â”‚  + Chunking   â”‚    â”‚  (FastEmbed/CPU)  â”‚             â”‚
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                  â”‚                     â”‚                       â”‚
                  â–¼                     â–¼                       â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
           â”‚   FastEmbed   â”‚    â”‚  Cosine Search    â”‚             â”‚
           â”‚  (bge-small)  â”‚    â”‚  Qdrant In-Memory â”‚             â”‚
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                  â”‚                     â”‚                       â”‚
                  â–¼                     â–¼                       â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Qdrant Store  â”‚    â”‚  System Prompt + Context + Query â”‚
           â”‚  (in-memory)  â”‚    â”‚         â†’ Google Gemini LLM      â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â–¼
                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                     â”‚   Answer +    â”‚
                                     â”‚  Page Cited   â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|---|---|
| Language | Python 3.10+ |
| Framework | LangChain |
| Frontend | Streamlit (ChatGPT-style UI) |
| Vector DB | Qdrant (in-memory for web app / Docker for CLI) |
| Embeddings | FastEmbed â€” `BAAI/bge-small-en-v1.5` (384-dim, CPU) |
| LLM | Google Gemini 2.0 Flash (free tier) |
| PDF Loader | PyPDF |

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ app.py                    # ğŸŒ Streamlit web UI (upload PDF + chat)
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ index.py              # CLI: Ingest PDF â†’ chunk â†’ embed â†’ Qdrant
â”‚   â”œâ”€â”€ chat.py               # CLI: Single-query RAG chat
â”‚   â”œâ”€â”€ chat_autorun.py       # CLI: Interactive multi-turn chat loop
â”‚   â””â”€â”€ docker-compose.yml    # One-command Qdrant setup (for CLI mode)
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml           # Streamlit theme (dark mode)
â”œâ”€â”€ .env.example              # Template for environment variables
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Option A: Web UI (Recommended)

```bash
git clone https://github.com/tejaswisinghparmar/RAG.git
cd RAG
python -m venv venv && source venv/bin/activate  # Windows: .\venv\Scripts\Activate.ps1
pip install -r requirements.txt
streamlit run app.py
```

Open **http://localhost:8501** â†’ paste your [free Gemini API key](https://aistudio.google.com/apikey) â†’ upload a PDF â†’ start chatting!

### Option B: CLI Mode (with Docker Qdrant)

```bash
# 1. Start Qdrant
cd rag && docker compose up -d && cd ..

# 2. Configure
cp .env.example rag/.env
# Edit rag/.env with your GOOGLE_API_KEY

# 3. Index a PDF
python rag/index.py

# 4. Chat
python rag/chat_autorun.py
```

---

## â˜ï¸ Free Deployment (Streamlit Community Cloud)

The web app needs **zero external services** â€” no database, no paid APIs.

| What | Where | Cost |
|---|---|---|
| Web App | [Streamlit Community Cloud](https://streamlit.io/cloud) | Free (public repos) |
| LLM | Users bring their own [Gemini key](https://aistudio.google.com/apikey) | Free for users |
| Vector DB | In-memory (no setup needed) | Free |
| Embeddings | FastEmbed (runs on CPU) | Free |

### Deploy in 3 Steps

1. **Push to GitHub** â€” Make sure your repo is public
2. **Go to [share.streamlit.io](https://share.streamlit.io/)** â†’ Connect your GitHub repo â†’ Set main file to `app.py`
3. **Done!** Share the URL on your resume. No secrets needed â€” users bring their own key.

---

## ğŸ”’ Security & Privacy

- **BYOK Model** â€” Users enter their own Gemini API key. Your key is never exposed.
- **No Persistent Storage** â€” PDFs are processed in-memory and discarded when the session ends.
- **API Keys Not Stored** â€” Keys exist only in the browser session state.
- **`.gitignore` Protection** â€” `.env` files and PDFs are excluded from version control.

---

## ğŸ’¡ How It Works

1. **Upload** â€” User uploads a PDF via the Streamlit sidebar.
2. **Chunk** â€” The PDF is split into overlapping chunks (~1 000 tokens each, 400-token overlap) to preserve context across page boundaries.
3. **Embed** â€” Each chunk is embedded using `bge-small-en-v1.5` (384-dim vectors) running locally on CPU via FastEmbed.
4. **Store** â€” Vectors are stored in an in-memory Qdrant instance (no external database).
5. **Retrieve** â€” When the user asks a question, the query is embedded and the top-4 most similar chunks are retrieved via cosine similarity.
6. **Generate** â€” Retrieved chunks + page numbers are injected into a system prompt, and Google Gemini generates a grounded, cited answer.

---

## ğŸ§  Challenges & Learnings

- **Chunking strategy matters** â€” Recursive splitting with 400-token overlap significantly improved retrieval accuracy for questions spanning multiple pages.
- **FastEmbed vs cloud embeddings** â€” Switched from cloud-based embedding APIs to FastEmbed for zero-cost, offline-capable, and faster indexing on CPU.
- **BYOK for free deployment** â€” Instead of burning through a shared API quota, letting users bring their own key makes the app sustainably free.
- **Prompt engineering for citations** â€” Explicitly instructing the LLM to cite page numbers reduced hallucinated answers and improved verifiability.
- **In-memory Qdrant** â€” Using `:memory:` mode eliminates the need for an external database in deployment while keeping the same LangChain API.

---

## ğŸ”® Future Scope

- **Agentic RAG** â€” Add tool-calling to let the LLM decide when to search, summarise, or ask for clarification
- **Hybrid Retrieval** â€” Combine dense (vector) + sparse (BM25) search for better recall
- **Reranking** â€” Cross-encoder reranker (e.g., `ms-marco-MiniLM`) for improved precision
- **Multi-document support** â€” Upload multiple PDFs and filter by source at retrieval time
- **Streaming responses** â€” Token-by-token output for a more responsive feel
- **Evaluation with RAGAS** â€” Measure faithfulness, answer relevance, and context precision
- **Chat history export** â€” Download conversation as PDF/Markdown

---

## ğŸ“œ License

This project is open-source under the [MIT License](LICENSE).

---

> Built with â¤ï¸ using LangChain, Qdrant, FastEmbed, Google Gemini, and Streamlit.
