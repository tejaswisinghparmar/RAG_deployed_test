# üìö PDF RAG Pipeline ‚Äî Document Q&A with Qdrant + LangChain + Gemini

> **Domain-specific Retrieval-Augmented Generation (RAG)** pipeline that ingests PDF documents, indexes them in a Qdrant vector database, and answers natural-language questions with page-level citations ‚Äî powered by Google Gemini and FastEmbed.

<!-- Replace with your own demo GIF / screenshot -->
<!-- ![Demo](docs/demo.gif) -->

---

## ‚ú® Key Features

| Feature | Details |
|---|---|
| **PDF Ingestion** | Automatic page-by-page loading via `PyPDFLoader` |
| **Smart Chunking** | Recursive character splitting (1 000 tokens, 400-token overlap) to preserve context across chunk boundaries |
| **Fast Embeddings** | `BAAI/bge-small-en-v1.5` (384-dim) via FastEmbed ‚Äî lightweight, runs on CPU |
| **Vector Storage** | Qdrant (Cosine similarity) with Docker for easy setup & persistence |
| **LLM Generation** | Google Gemini (`gemini-2.5-flash-lite` / `gemini-2.0-flash-lite`) with grounded, citation-aware prompts |
| **Two Chat Modes** | Single-query mode (`chat.py`) and interactive multi-turn loop (`chat_autorun.py`) |
| **Page Citations** | Every answer references the source page number so you can verify in the original PDF |

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PDF Doc  ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ  Chunking   ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ  FastEmbed    ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ Qdrant  ‚îÇ    ‚îÇ   Gemini   ‚îÇ
‚îÇ (PyPDF)   ‚îÇ   ‚îÇ (Recursive) ‚îÇ   ‚îÇ bge-small-en  ‚îÇ   ‚îÇ VectorDB‚îÇ    ‚îÇ    LLM     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                          ‚îÇ               ‚îÇ
                                                          ‚ñº               ‚îÇ
                                                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
                                          Query ‚îÄ‚îÄ‚ñ∂‚îÇ  Retriever  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ
                                                   ‚îÇ  (top-k=4)  ‚îÇ  context  ‚îÄ‚îÄ‚ñ∂ Answer
                                                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Tech Stack

| Layer | Technology |
|---|---|
| Language | Python 3.10+ |
| Framework | LangChain |
| Vector DB | Qdrant (Docker) |
| Embeddings | FastEmbed (`BAAI/bge-small-en-v1.5`, 384-dim) |
| LLM | Google Gemini (Flash Lite) |
| PDF Loader | PyPDF |
| Environment | python-dotenv |

---

## üìÇ Project Structure

```
.
‚îú‚îÄ‚îÄ rag/
‚îÇ   ‚îú‚îÄ‚îÄ index.py              # Ingestion ‚Äî load PDF, chunk, embed, store in Qdrant
‚îÇ   ‚îú‚îÄ‚îÄ chat.py               # Single-query RAG chat
‚îÇ   ‚îú‚îÄ‚îÄ chat_autorun.py       # Interactive multi-turn RAG chat loop
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml    # One-command Qdrant setup
‚îú‚îÄ‚îÄ .env.example              # Template for required environment variables
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üöÄ Setup & Run

### Prerequisites

- **Python 3.10+**
- **Docker** (for Qdrant)
- A **Google Gemini API key** ‚Äî get one free at [aistudio.google.com/apikey](https://aistudio.google.com/apikey)

### 1. Clone the repo

```bash
git clone https://github.com/tejaswisinghparmar/RAG.git
cd RAG
```

### 2. Create & activate a virtual environment

```bash
python -m venv venv

# Windows
.\venv\Scripts\Activate.ps1

# macOS / Linux
source venv/bin/activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure environment variables

```bash
cp .env.example rag/.env
```

Open `rag/.env` and paste your Google API key:

```
GOOGLE_API_KEY="your_actual_key_here"
```

### 5. Start Qdrant (vector database)

```bash
cd rag
docker compose up -d
```

Qdrant dashboard will be available at **http://localhost:6333/dashboard**.

### 6. Index your PDF

Place your PDF file in the `rag/` folder (update `PDF_FILENAME` in `index.py` if needed), then:

```bash
python rag/index.py
```

### 7. Chat with your document

**Single question:**
```bash
python rag/chat.py
```

**Interactive session (multi-turn):**
```bash
python rag/chat_autorun.py
```

---

## üí° How It Works

1. **Ingestion (`index.py`)** ‚Äî The PDF is loaded page-by-page, split into overlapping chunks of ~1 000 tokens, embedded with `bge-small-en-v1.5`, and stored in a Qdrant collection.
2. **Retrieval** ‚Äî When the user asks a question, the query is embedded and the top-k most similar chunks are retrieved from Qdrant using cosine similarity.
3. **Generation** ‚Äî The retrieved chunks (with page numbers) are injected into a system prompt, and Google Gemini generates a grounded answer with page citations.

---

## üß† Challenges & Learnings

- **Chunking strategy matters** ‚Äî Recursive splitting with 400-token overlap significantly improved retrieval accuracy for questions spanning two pages.
- **FastEmbed vs cloud embeddings** ‚Äî Switched from cloud-based embedding APIs to FastEmbed for zero-cost, offline-capable, and faster indexing on CPU.
- **Prompt engineering for citation** ‚Äî Explicitly instructing the LLM to cite page numbers reduced hallucinated answers and improved verifiability.

---

## üîÆ Future Scope

- **Agentic RAG** ‚Äî Add tool-calling to let the LLM decide when to search, summarise, or ask for clarification.
- **Hybrid Retrieval** ‚Äî Combine dense (vector) + sparse (BM25) search for better recall.
- **Reranking** ‚Äî Add a cross-encoder reranker (e.g., `ms-marco-MiniLM`) to improve precision after retrieval.
- **Streamlit / Gradio UI** ‚Äî Web interface for a more polished demo experience.
- **Multi-document support** ‚Äî Ingest multiple PDFs and filter by source at retrieval time.
- **Evaluation with RAGAS** ‚Äî Measure faithfulness, answer relevance, and context precision.

---

## üìú License

This project is open-source under the [MIT License](LICENSE).

---

> Built with ‚ù§Ô∏è using LangChain, Qdrant, FastEmbed, and Google Gemini.
